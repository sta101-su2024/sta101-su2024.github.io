{
  "hash": "747e36113d9ecac10d3acba7a544103c",
  "result": {
    "markdown": "---\ntitle: Regression with multiple predictors\nsubtitle: Lecture 7\nformat: revealjs\nauto-stretch: false\n---\n\n\n# Warm up\n\n## Check-in\n\n-   Daily check-ins for getting you thinking at the beginning of the class and reviewing recent/important concepts\n-   Go to Canvas and open today's \"quiz\" titled `2023-09-25 Check-in`\n-   Access code: `___` (released in class) <!-- influential point -->\n-   \"Graded\" for completion\n\n## Announcements {.smaller}\n\n-   *Suggested answers* for application exercises posted\n-   My office hours this week:\n    -   Tuesday 4:45-5:45 in Old Chem 213 (instead of the usual 3:30 - 5:30)\n    -   Wednesday 5-6 on Zoom\n-   Lab 4 is due Thu, Sep 28 at 5 pm on Gradescope\n-   Lab this week: Exam 1 review\n-   Exam 1:\n    -   In class - Wed, Oct 4\n    -   Take home - Starts after in-class exam, ends Fri, Oct 6 at 5pm\n    -   No lab on Fri, Oct 6\n\n## Load packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\n```\n:::\n\n\n# $R^2$ and model fit\n\n## $R^2$\n\n$R^2$, aka \"the coefficient of determination\" or \"correlation squared\" is a way to see how well a given model fits the data.\n\n$$\nR^2 = r_i^2 \n$$\n\n## Sum of squares total, SST\n\nThe sum of squares total is a measure of the total variability in the outcome variable:\n\n$$\nSST = (y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\cdots + (y_n - \\bar{y})^2\n$$\n\n## Sum of squares residuals, SSE\n\nThe sum of squares residuals (error) is a measure of the variability in the residuals, i.e., variability left unexplained in the outcome variable after the model is fit:\n\n$$\nSSE = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots + (y_n - \\hat{y}_n)^2\n$$\n\n## $R^2$, another look\n\n$$\nR^2 = \\frac{SST - SSE}{SST} = 1 - \\frac{SSE}{SST}\n$$\n\n. . .\n\n-   This can be summarized as \"$R^2$ is 1 minus the sum of squared residuals divided by the sum of squared total\".\n\n. . .\n\n-   In other words, $R^2$ is the proportion of variability in the outcome that is unexplained by the model.\n\n## $R^2$\n\n::: incremental\n-   If the sum of squared residuals is 0, then the model explains all variability and $R^2 = 1 - 0 = 1$.\n\n-   If the sum of squared residuals is the same as all the variability in the data, then model does not explain any variability and $R^2 = 1 - 1 = 0$.\n\n-   $R^2$ is a measure of the proportion of variability the model explains. An $R^2$ of 0 is a poor fit and $R^2$ of 1 is a perfect fit.\n:::\n\n## Finding $R^2$\n\nTo find $R^2$ simply call the function `glance()` on your `model_fit`, e.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_fit = linear_reg() |>\n  fit(outcome ~ predictor, data = data_set)\n  \nglance(model_fit)\n```\n:::\n\n\n# Models with multiple predictors\n\n## Two predictors {.smaller}\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n$$\n\n::: incremental\n-   $y$: the **outcome** variable. Also called the \"response\" or \"dependent variable\". In prediction problems, this is what we are interested in predicting.\n-   $x_i$: the $i^{th}$ **predictor**. Also commonly referred to as \"regressor\", \"independent variable\", \"covariate\", \"feature\", \"the data\".\n-   $\\beta_i$: \"constants\" or **coefficients** i.e. fixed numbers. These are **population parameters**. $\\beta_0$ has another special name, \"the intercept\".\n-   $\\epsilon$: the **error**. This quantity represents observational error, i.e. the difference between our observation and the true population-level expected value: $\\beta_0 + \\beta_1 x$.\n-   Effectively this model says our data $y$ is linearly related to the $x_1$ and $x_2$ but is not perfectly observed due to unexplained errors.\n:::\n\n## A simple example\n\nLet's examine the first quarter of 2020 high prices of Microsoft, IBM, and Apple stocks to illustrate some ideas.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-regression-multiple-predictors-1_files/figure-revealjs/simple-example-1.png){width=960}\n:::\n:::\n\n\n-   If we have three measurements (variables) then each observation is a point in three-dimensional space. In this example, we can choose one of our measurements to be the outcome variable (e.g. Apple stock price) and use our other two measurements (MSFT and IBM price) as predictors.\n\n-   In general, the total number of measurements, i.e. variables (columns) in our linear model represents the spatial dimension of our model.\n\n## 2 predictors + 1 outcome = 3 dimensions\n\nThe fitted linear model no longer looks like a line, but instead looks like a **plane**. It shows our prediction of AAPL price ($y$) given both MSFT price ($x_1$) and IBM price ($x_2$).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-regression-multiple-predictors-1_files/figure-revealjs/simple-example-plane-1.png){width=960}\n:::\n:::\n\n\n## Fitting a multiple regression model in R\n\nFind the **equation of the plane** by adding in new predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_model_fit <- linear_reg() |>\n  fit(outcome ~ predictor1 + predictor2 + predictor3 + ..., data = data_frame)\n```\n:::\n\n\n. . .\n\n-   This code template will fit the model according to the ordinary least squares (OLS) objective function, i.e., we are finding the equation of the hyperplane that minimizes the sum of squared residuals\n\n. . .\n\n-   You can then display a tidy output of the model with the `tidy()` function on your fitted model: `tidy(my_model_fit)`\n\n## Application exercise\n\nGo to Posit Cloud and start the project titled ae-07-Books.\n",
    "supporting": [
      "07-regression-multiple-predictors-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}